Great — below is a complete, copy-pasteable step-by-step manual for your setup and training flow using Conda (no sudo) + ffmpeg (conda-forge), and full code files you asked for: training.py (rewritten, ready to run), extract_frames.sh, reconstruct_from_ckpt.py, and verify_metrics.py. Follow the steps in order from top to bottom.

I assume your repo root is DCVC/ and you have:

training video: DCVC/data/train_videos/clip_01.mp4

validation video(s) (optional) folder: DCVC/data/val_videos/


Everything here will work without sudo.


---

1 — Create Conda env & install packages (no sudo)

Open a terminal in the DCVC/ repo root and run:

# 1. create & activate environment
conda create -n dcvc python=3.12 -y
conda activate dcvc

# 2. install ffmpeg and common packages from conda-forge (no sudo)
conda install -c conda-forge ffmpeg -y

# 3. install Python dependencies (PyTorch index below: pick correct CUDA for your machine)
# Example: for CUDA 12.6, use the following (change if you need a different CUDA)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126

# Install other python packages
pip install numpy pillow tqdm scikit-image tensorboard opencv-python decord

# (Optional) GPU-accelerated decord is fine; decord CPU fallback will work if not available.

If your GPU/CUDA differs, install the matching torch wheel per your system. The rest of the tools are conda/pip and do not need sudo.


---

2 — Frame extraction (use ffmpeg)

Run the script below (create file and execute) to extract frames of clip_01.mp4 to PNGs. This step is recommended for speed during training.

Create extract_frames.sh in repo root (DCVC/extract_frames.sh):

#!/bin/bash
# extract_frames.sh
# Usage: ./extract_frames.sh <input_video> <output_dir> <fps> <width> <height>
# Example: ./extract_frames.sh data/train_videos/clip_01.mp4 data/frames/train/clip_01 60 1920 1080

IN_VIDEO="$1"
OUT_DIR="$2"
FPS="${3:-60}"
W="${4:-1920}"
H="${5:-1080}"

if [ -z "$IN_VIDEO" ] || [ -z "$OUT_DIR" ]; then
  echo "Usage: $0 <input_video> <output_dir> <fps> <width> <height>"
  exit 1
fi

mkdir -p "$OUT_DIR"
echo "Extracting frames from $IN_VIDEO -> $OUT_DIR at ${FPS}fps, ${W}x${H}..."
ffmpeg -hide_banner -y -i "$IN_VIDEO" -r "$FPS" -vf "scale=${W}:${H}" -q:v 2 "$OUT_DIR/%06d.png"
echo "Done."

Make it executable and run:

chmod +x extract_frames.sh
# create destination folder and run
./extract_frames.sh data/train_videos/clip_01.mp4 data/frames/train/clip_01 60 1920 1080

Frames will appear under DCVC/data/frames/train/clip_01/000001.png, etc.

If you also have validation videos, repeat for them into data/frames/val/<name>.


---

3 — (Optional) Quick train/val split if you only have one clip

If you only used clip_01 and want a tiny validation set, move ~10% frames to data/frames/val/clip_01:

mkdir -p data/frames/val/clip_01
python - <<'PY'
import os, shutil, random
src = "data/frames/train/clip_01"
dst = "data/frames/val/clip_01"
os.makedirs(dst, exist_ok=True)
files = sorted([f for f in os.listdir(src) if f.endswith(".png")])
n = max(1, int(0.1 * len(files)))
chosen = random.sample(files, n)
for f in chosen:
    shutil.move(os.path.join(src, f), os.path.join(dst, f))
print("Moved", len(chosen), "frames to", dst)
PY


---

4 — training.py (full code) — put in repo root DCVC/training.py

This training script is self-contained, robust to model output differences, supports mixed precision, and works with pre-extracted frames (Option A). Copy & paste the entire block into DCVC/training.py.

#!/usr/bin/env python3
"""
training.py - Training loop for DCVC-style models on pre-extracted frames.

Usage (single GPU):
  python training.py --train-glob "data/frames/train/**/*.png" --val-glob "data/frames/val/**/*.png" \
    --patch-size 256 --temporal-len 4 --batch-size 6 --epochs 30 --cuda --amp \
    --pretrained ./checkpoints/cvpr2025_video.pth.tar --lambda-rd 0.01 --save-dir ./checkpoints_finetune

If your repo's VideoModel constructor signature differs, edit the import/constructor near top.
"""
import argparse, os, math, random, time
from glob import glob
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import GradScaler, autocast

import numpy as np
from PIL import Image

# Try import model classes from repo - adapt if names differ
try:
    from src.models.video_model import DMC as VideoModel
except Exception:
    VideoModel = None

# ----------------------------
# Dataset - reads PNG frames list (flat) and samples temporal clips + random crop patches
# ----------------------------
class SequencePatchDataset(Dataset):
    def __init__(self, file_list, seq_len=4, patch_size=256, augment=True):
        """
        file_list: sorted list of all frame file paths across videos
        We treat file_list as a timeline; sampling is a sliding window with random start.
        """
        self.files = sorted(file_list)
        self.seq_len = seq_len
        self.patch_size = patch_size
        self.augment = augment
        if len(self.files) == 0:
            raise ValueError("No frame files provided to SequencePatchDataset")

    def __len__(self):
        return max(1, len(self.files) // max(1, self.seq_len))

    def _read_seq(self, start_idx):
        idxs = [min(len(self.files)-1, start_idx + i) for i in range(self.seq_len)]
        imgs = []
        for i in idxs:
            p = self.files[i]
            im = Image.open(p).convert('RGB')
            arr = np.asarray(im, dtype=np.float32) / 255.0
            imgs.append(arr)
        seq = np.stack(imgs, axis=0) # [T,H,W,3]
        return seq

    def __getitem__(self, idx):
        max_start = max(0, len(self.files) - self.seq_len)
        start = random.randint(0, max_start) if max_start > 0 else 0
        seq = self._read_seq(start)  # [T,H,W,3]
        T,H,W,C = seq.shape
        ps = self.patch_size
        if H < ps or W < ps:
            # resize frames up if needed
            from PIL import Image
            seq = np.stack([np.asarray(Image.fromarray((f*255).astype(np.uint8)).resize((max(W,ps), max(H,ps))), dtype=np.float32)/255.0 for f in seq], axis=0)
            T,H,W,C = seq.shape
        x = random.randint(0, W-ps) if W > ps else 0
        y = random.randint(0, H-ps) if H > ps else 0
        seq = seq[:, y:y+ps, x:x+ps, :]  # [T,ps,ps,3]
        if self.augment:
            if random.random() < 0.5:
                seq = seq[:, :, ::-1, :]
            if random.random() < 0.5:
                seq = seq[:, ::-1, :, :]
        # to tensor [T,3,H,W]
        seq = seq.transpose(0,3,1,2).copy()
        seq = torch.from_numpy(seq).float()
        return seq

# ----------------------------
# bpp helper
# ----------------------------
def compute_bpp_from_likelihoods(likelihoods, num_pixels):
    if likelihoods is None:
        return torch.tensor(0.0)
    if isinstance(likelihoods, dict):
        tensors = [v for v in likelihoods.values() if isinstance(v, torch.Tensor)]
    elif isinstance(likelihoods, (list, tuple)):
        tensors = [t for t in likelihoods if isinstance(t, torch.Tensor)]
    elif isinstance(likelihoods, torch.Tensor):
        tensors = [likelihoods]
    else:
        try:
            return torch.tensor(float(likelihoods))
        except Exception:
            return torch.tensor(0.0)
    total_bits = torch.tensor(0.0, device=tensors[0].device)
    for t in tensors:
        p = torch.clamp(t, min=1e-9)
        total_bits = total_bits + (-torch.sum(torch.log(p)) / math.log(2.0))
    bpp = total_bits / float(num_pixels)
    return bpp

# ----------------------------
# training/validation epoch
# ----------------------------
def run_epoch(model, loader, optimizer, scaler, device, args, epoch, is_train=True):
    model.train() if is_train else model.eval()
    total_loss = 0.0
    total_dist = 0.0
    total_bpp = 0.0
    steps = 0
    startt = time.time()

    for it, seq in enumerate(loader):
        # seq: [B,T,3,H,W]
        seq = seq.to(device, non_blocking=True)
        B,T,C,H,W = seq.shape
        num_pixels = B * H * W  # normalized per-frame; adjust if needed
        try:
            with autocast(enabled=args.amp):
                out = model(seq)  # most video models accept [B,T,C,H,W]
        except Exception:
            # fallback: run as flattened frames with image model
            flat = seq.view(B*T, C, H, W)
            with autocast(enabled=args.amp):
                out_flat = model(flat)
            # attempt to parse outputs
            if isinstance(out_flat, dict):
                x_hat_flat = out_flat.get('x_hat') or out_flat.get('recon') or out_flat.get('x_rec')
                like_flat = out_flat.get('likelihoods') or out_flat.get('y_likelihoods') or out_flat.get('lik')
            elif isinstance(out_flat, (list, tuple)):
                x_hat_flat = out_flat[0]
                like_flat = out_flat[1] if len(out_flat) > 1 else None
            else:
                x_hat_flat = None
                like_flat = None
            if x_hat_flat is not None and x_hat_flat.dim() == 4 and x_hat_flat.shape[0] == B*T:
                x_hat = x_hat_flat.view(B, T, C, H, W)
            else:
                x_hat = seq.clone()
            likelihoods = like_flat
            out = {'x_hat': x_hat, 'likelihoods': likelihoods}

        # normalize output
        if isinstance(out, dict):
            x_hat = out.get('x_hat') or out.get('recon') or out.get('x_rec')
            likelihoods = out.get('likelihoods') or out.get('y_likelihoods') or out.get('lik')
            if x_hat is not None and x_hat.dim() == 4 and x_hat.shape[0] == B*T:
                x_hat = x_hat.view(B, T, C, H, W)
        elif isinstance(out, (list, tuple)):
            x_hat = out[0]
            likelihoods = out[1] if len(out) > 1 else None
            if x_hat is not None and x_hat.dim() == 4 and x_hat.shape[0] == B*T:
                x_hat = x_hat.view(B, T, C, H, W)
        else:
            raise RuntimeError("Unexpected model output type")

        if x_hat is None:
            x_hat = seq.clone()

        # MSE over all frames and channels
        dist = nn.functional.mse_loss(x_hat, seq, reduction='mean')
        bpp = compute_bpp_from_likelihoods(likelihoods, num_pixels).to(dist.device)
        loss = dist + args.lambda_rd * bpp

        if is_train:
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            if args.max_norm > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)
            scaler.step(optimizer)
            scaler.update()

        total_loss += float(loss.detach().cpu().item())
        total_dist += float(dist.detach().cpu().item())
        total_bpp += float(bpp.detach().cpu().item())
        steps += 1

        if is_train and it % args.log_interval == 0:
            print(f"{'Train' if is_train else 'Val'} Epoch {epoch} it {it}/{len(loader)} loss={loss.item():.6f} dist={dist.item():.6f} bpp={bpp.item():.6f}")

    elapsed = time.time() - startt
    avg_loss = total_loss / max(1, steps)
    avg_dist = total_dist / max(1, steps)
    avg_bpp = total_bpp / max(1, steps)
    print(f"{'Train' if is_train else 'Val'} Epoch {epoch} finished: avg_loss={avg_loss:.6f} avg_dist={avg_dist:.6f} avg_bpp={avg_bpp:.6f} throughput_steps_per_sec={steps/max(1e-6,elapsed):.2f}")
    return avg_loss, avg_dist, avg_bpp

# ----------------------------
# main
# ----------------------------
def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--train-glob', type=str, required=True)
    parser.add_argument('--val-glob', type=str, default='')
    parser.add_argument('--patch-size', type=int, default=256)
    parser.add_argument('--temporal-len', type=int, default=4)
    parser.add_argument('--batch-size', type=int, default=6)
    parser.add_argument('--epochs', type=int, default=30)
    parser.add_argument('--lr', type=float, default=2e-4)
    parser.add_argument('--weight-decay', type=float, default=1e-6)
    parser.add_argument('--lambda-rd', type=float, default=0.01)
    parser.add_argument('--pretrained', type=str, default='')
    parser.add_argument('--save-dir', type=str, default='./checkpoints_finetune')
    parser.add_argument('--log-dir', type=str, default='./logs')
    parser.add_argument('--cuda', action='store_true')
    parser.add_argument('--amp', action='store_true')
    parser.add_argument('--workers', type=int, default=4)
    parser.add_argument('--max-norm', type=float, default=1.0)
    parser.add_argument('--log-interval', type=int, default=50)
    args = parser.parse_args()

    device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')
    print("Device:", device)

    # load file lists
    train_files = sorted(glob(args.train_glob, recursive=True))
    val_files = sorted(glob(args.val_glob, recursive=True)) if args.val_glob else []
    print("Train frames:", len(train_files), "Val frames:", len(val_files))

    train_ds = SequencePatchDataset(train_files, seq_len=args.temporal_len, patch_size=args.patch_size, augment=True)
    train_loader = DataLoader(train_ds, batch_size=args.batch_size, shuffle=True, num_workers=args.workers, pin_memory=True, drop_last=True)

    val_loader = None
    if len(val_files) > 0:
        val_ds = SequencePatchDataset(val_files, seq_len=args.temporal_len, patch_size=args.patch_size, augment=False)
        val_loader = DataLoader(val_ds, batch_size=max(1, args.batch_size//2), shuffle=False, num_workers=max(1,args.workers//2), pin_memory=True)

    # instantiate model
    if VideoModel is None:
        raise RuntimeError("VideoModel import failed. Edit training.py to import the correct class from src.models.")
    model = VideoModel()
    model.to(device)

    # load pretrained (optional)
    if args.pretrained and os.path.exists(args.pretrained):
        ck = torch.load(args.pretrained, map_location='cpu')
        sd = ck.get('state_dict', ck)
        try:
            model.load_state_dict(sd, strict=False)
            print("Loaded pretrained checkpoint:", args.pretrained)
        except Exception as e:
            print("Warning: load_state_dict strict failed:", e)
            # try to load partial keys
            model.load_state_dict({k.replace('module.',''):v for k,v in sd.items() if isinstance(v, torch.Tensor)}, strict=False)

    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)
    scaler = GradScaler(enabled=args.amp)

    best_val = 1e9
    os.makedirs(args.save_dir, exist_ok=True)

    for epoch in range(args.epochs):
        train_loss, train_dist, train_bpp = run_epoch(model, train_loader, optimizer, scaler, device, args, epoch, is_train=True)
        if val_loader is not None:
            val_loss, val_dist, val_bpp = run_epoch(model, val_loader, optimizer, scaler, device, args, epoch, is_train=False)
            # save best
            if val_loss < best_val:
                best_val = val_loss
                fn = os.path.join(args.save_dir, f"best_epoch{epoch:03d}.pth.tar")
                torch.save({'epoch':epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, fn)
                print("Saved best checkpoint:", fn)

        # periodic save
        if (epoch + 1) % 5 == 0:
            fn = os.path.join(args.save_dir, f"epoch{epoch:03d}.pth.tar")
            torch.save({'epoch':epoch, 'state_dict': model.state_dict(), 'optimizer': optimizer.state_dict()}, fn)
            print("Saved checkpoint:", fn)

    print("Training finished.")

if __name__ == "__main__":
    main()

Notes on training.py:

If src.models.video_model.DMC is named differently in your repo, open src/models/ and adjust the import.

The script expects pre-extracted PNG frames; --train-glob "data/frames/train/**/*.png".

Check --pretrained path if you want to fine-tune from a provided checkpoint.



---

5 — Run training (single GPU)

From repo root (DCVC/) execute:

python training.py \
  --train-glob "data/frames/train/**/*.png" \
  --val-glob "data/frames/val/**/*.png" \
  --patch-size 256 --temporal-len 4 --batch-size 6 \
  --epochs 30 --cuda --amp \
  --pretrained "./checkpoints/cvpr2025_video.pth.tar" \
  --lambda-rd 0.01 \
  --save-dir "./checkpoints_finetune_lambda0.01" \
  --log-dir "./logs/dcvc_finetune_lambda0.01" \
  --workers 4

Adjust --batch-size if OOM. If no pretrained checkpoint available, remove --pretrained and training will start from random.

If you want to run from a notebook as a subprocess and stream logs, use the cell approach I provided earlier (subprocess.Popen).


---

6 — Monitor training with TensorBoard

Start TensorBoard (in another terminal; still inside env):

tensorboard --logdir ./logs/dcvc_finetune_lambda0.01 --port 6006 --host 127.0.0.1

Open http://127.0.0.1:6006 in your browser.


---

7 — Inference: reconstruct frames from your trained checkpoint

Create reconstruct_from_ckpt.py in repo root. This script loads a saved checkpoint (best/epoch) and reconstructs frames from a source frame folder, saving reconstructed PNGs so you can compute PSNR/SSIM.

#!/usr/bin/env python3
"""
reconstruct_from_ckpt.py
Usage:
  python reconstruct_from_ckpt.py --ckpt ./checkpoints_finetune_lambda0.01/best_epoch000.pth.tar \
    --input_dir data/frames/test/clip_01 --out_dir eval_out/recon_clip01 --batch_size 1 --cuda

This script assumes the model's forward can accept flattened frames or sequences.
It will attempt to call model on each frame individually (safe fallback).
"""
import argparse, os, math
from glob import glob
from PIL import Image
import numpy as np
import torch

try:
    from src.models.video_model import DMC as VideoModel
except Exception:
    VideoModel = None

def load_image(path):
    im = Image.open(path).convert('RGB')
    arr = np.asarray(im, dtype=np.float32) / 255.0
    arr = torch.from_numpy(arr.transpose(2,0,1)).unsqueeze(0)  # [1,3,H,W]
    return arr

def save_image(tensor, path):
    # tensor: [1,3,H,W] or [3,H,W]
    if tensor.dim() == 4:
        tensor = tensor[0]
    arr = tensor.detach().cpu().clamp(0,1).numpy().transpose(1,2,0) * 255.0
    im = Image.fromarray(arr.astype('uint8'))
    im.save(path)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument('--ckpt', required=True)
    parser.add_argument('--input_dir', required=True)
    parser.add_argument('--out_dir', required=True)
    parser.add_argument('--batch_size', type=int, default=1)
    parser.add_argument('--cuda', action='store_true')
    args = parser.parse_args()

    device = torch.device('cuda' if args.cuda and torch.cuda.is_available() else 'cpu')
    if VideoModel is None:
        raise RuntimeError("Could not import VideoModel. Edit code to import correct model class.")
    model = VideoModel()
    ck = torch.load(args.ckpt, map_location='cpu')
    sd = ck.get('state_dict', ck)
    try:
        model.load_state_dict(sd, strict=False)
    except Exception:
        # try stripping module.
        model.load_state_dict({k.replace('module.',''):v for k,v in sd.items() if isinstance(v, torch.Tensor)}, strict=False)
    model.to(device)
    model.eval()

    os.makedirs(args.out_dir, exist_ok=True)
    files = sorted(glob(os.path.join(args.input_dir, "*.png")))
    print("Frames to process:", len(files))
    with torch.no_grad():
        for i, p in enumerate(files):
            img = load_image(p).to(device)  # [1,3,H,W]
            # try sequence call (1-frame sequence)
            try:
                # many video models expect [B,T,C,H,W], try that
                seq = img.unsqueeze(1)  # [1,1,3,H,W]
                out = model(seq)
                if isinstance(out, dict):
                    x_hat = out.get('x_hat') or out.get('recon') or out.get('x_rec')
                elif isinstance(out, (list,tuple)):
                    x_hat = out[0]
                else:
                    x_hat = None
                if x_hat is None:
                    # fallback to flat image output
                    raise RuntimeError("No x_hat from sequence forward")
                # x_hat may be [1,1,3,H,W] or [1,3,H,W]
                if x_hat.dim() == 5:
                    x_img = x_hat[:,0]
                else:
                    x_img = x_hat
            except Exception:
                # fallback: try image forward
                try:
                    out2 = model(img)
                    if isinstance(out2, dict):
                        x_hat = out2.get('x_hat') or out2.get('recon') or out2.get('x_rec')
                    elif isinstance(out2, (list,tuple)):
                        x_hat = out2[0]
                    else:
                        x_hat = None
                    if x_hat is None:
                        x_img = img
                    elif x_hat.dim() == 4 and x_hat.shape[0] == 1:
                        x_img = x_hat
                    else:
                        x_img = img
                except Exception:
                    x_img = img
            out_path = os.path.join(args.out_dir, f"{i+1:06d}.png")
            save_image(x_img.cpu(), out_path)
            if (i+1) % 50 == 0:
                print("Saved", i+1, "frames")

    print("Reconstruction finished. Saved to", args.out_dir)

if __name__ == "__main__":
    main()

Make it executable:

chmod +x reconstruct_from_ckpt.py

Run it (example):

python reconstruct_from_ckpt.py \
  --ckpt ./checkpoints_finetune_lambda0.01/best_epoch000.pth.tar \
  --input_dir data/frames/test/clip_01 \
  --out_dir eval_out/recon_clip01 \
  --cuda

This will write eval_out/recon_clip01/000001.png etc.


---

8 — Compute PSNR & SSIM (verify quality)

Create verify_metrics.py in repo root (or run inline). This compares original frames and reconstructed frames and reports averages.

#!/usr/bin/env python3
# verify_metrics.py
import os, glob
import numpy as np
from PIL import Image
from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim

def load_img(path):
    im = Image.open(path).convert('RGB')
    return np.array(im, dtype=np.uint8)

def compute_metrics(orig_dir, recon_dir):
    origs = sorted(glob.glob(os.path.join(orig_dir, "*.png")))
    recons = sorted(glob.glob(os.path.join(recon_dir, "*.png")))
    n = min(len(origs), len(recons))
    if n == 0:
        raise RuntimeError("No images found to compare.")
    ps = []
    ss = []
    for i in range(n):
        a = load_img(origs[i])
        b = load_img(recons[i])
        ps.append(psnr(a, b, data_range=255))
        ss.append(ssim(a, b, multichannel=True, data_range=255))
    import statistics
    print("Compared frames:", n)
    print("Avg PSNR: {:.3f} dB".format(statistics.mean(ps)))
    print("Avg SSIM: {:.4f}".format(statistics.mean(ss)))
    return ps, ss

if __name__ == "__main__":
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--orig', required=True, help='original frames dir')
    parser.add_argument('--recon', required=True, help='reconstructed frames dir')
    args = parser.parse_args()
    compute_metrics(args.orig, args.recon)

Run:

python verify_metrics.py --orig data/frames/test/clip_01 --recon eval_out/recon_clip01

It prints average PSNR (dB) and SSIM.


---

9 — (Optional) VMAF — perceptual metric

You installed ffmpeg via conda-forge so you may also need libvmaf available in that ffmpeg build. If present, run:

# create recon video and original video from frames (if needed) - example for recon
ffmpeg -y -r 60 -i eval_out/recon_clip01/%06d.png -c:v libx264 -pix_fmt yuv420p eval_out/recon_clip01.mp4
ffmpeg -y -r 60 -i data/frames/test/clip_01/%06d.png -c:v libx264 -pix_fmt yuv420p eval_out/orig_clip01.mp4

# compute vmaf
ffmpeg -i eval_out/recon_clip01.mp4 -i eval_out/orig_clip01.mp4 -lavfi libvmaf="log_path=eval_out/vmaf.json" -f null -

If ffmpeg's build does not include libvmaf, skip this; PSNR/SSIM are still valid.


---

10 — Quick troubleshooting & tips

If training training.py raises VideoModel import failed, open src/models/ and find the class name or file. Edit the import at top of training.py:

e.g., from src.models.your_video_file import YourModelClass as VideoModel


OOM: reduce --batch-size or --patch-size and enable --amp.

If compute_bpp_from_likelihoods returns zero: the model's likelihoods format differs. Inspect model forward outputs by running a small forward on a single patch to see keys.

If reconstruct_from_ckpt.py produces nearly identical reconstructions to input (i.e., very high PSNR early), that's normal if you fine-tuned briefly; use verify_metrics.py to quantify change across epochs.

If you want me to adapt training.py exactly to the DMC forward signature in your repo, paste the first 200 lines of src/models/video_model.py here and I’ll update imports/calls to match.



---

11 — Summary of exact commands to run (copy-paste sequence)

From DCVC/ repo root:

# 1. create env and install packages (do this once)
conda create -n dcvc python=3.12 -y
conda activate dcvc
conda install -c conda-forge ffmpeg -y
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu126
pip install numpy pillow tqdm scikit-image tensorboard opencv-python decord

# 2. place provided files into repo root:
#   - training.py
#   - extract_frames.sh
#   - reconstruct_from_ckpt.py
#   - verify_metrics.py
# (You can paste/save them as shown above)

# 3. extract frames from clip_01.mp4
./extract_frames.sh data/train_videos/clip_01.mp4 data/frames/train/clip_01 60 1920 1080

# 4. optionally split validation frames
# (run the python snippet earlier if desired)

# 5. run training (single GPU)
python training.py \
  --train-glob "data/frames/train/**/*.png" \
  --val-glob "data/frames/val/**/*.png" \
  --patch-size 256 --temporal-len 4 --batch-size 6 \
  --epochs 30 --cuda --amp \
  --pretrained "./checkpoints/cvpr2025_video.pth.tar" \
  --lambda-rd 0.01 \
  --save-dir "./checkpoints_finetune_lambda0.01" \
  --log-dir "./logs/dcvc_finetune_lambda0.01" \
  --workers 4

# 6. reconstruct frames from best checkpoint
python reconstruct_from_ckpt.py --ckpt ./checkpoints_finetune_lambda0.01/best_epoch000.pth.tar \
   --input_dir data/frames/test/clip_01 --out_dir eval_out/recon_clip01 --cuda

# 7. compute PSNR/SSIM
python verify_metrics.py --orig data/frames/test/clip_01 --recon eval_out/recon_clip01


---

If you want, I can do one of these next, right now in this chat:

(A) paste the training.py file again with small edits if your model class has a different name (tell me the class name or paste src/models/video_model.py top).

(B) create a minimal dataset_config_gaming.json for test_video.py (if you plan to use repo's test_video.py instead of the reconstruct script).

(C) produce a short notebook (.ipynb) that contains the commands/cells above, ready to run.


Tell me which (A/B/C) and I’ll provide it immediately — or if you’re ready, run the steps above and come back with any error logs and I’ll debug them with you.

