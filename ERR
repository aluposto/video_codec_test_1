Nice — you’re almost there. The runpy.run_path(...) call executed the script and returned its globals (that blob you pasted). The script printed cannot import cuda implementation for inference, fallback to pytorch. (that’s OK — it will use the Python fallback implementation), but it did not actually run the script’s main() (the top-level run used runpy.run_path which executed top-level code, but in this repo test_video.py mostly defines functions and main() — you need to call it).

Below are two ready-to-run notebook cells you can paste into your IPYNB.

1. The first cell ensures the repo root is on sys.path and then runs main() from test_video.py with your command-line args (same args you used earlier). This will execute the full test script inside the notebook and stream output into the cell.


2. The second cell is optional: it attempts to compile the CUDA inference extension in-place (so you get the faster native implementation). Only run it if you have a working CUDA toolchain (nvcc) and development headers — otherwise the build will fail and you can keep using the PyTorch fallback.




---

Cell A — run test_video.py main() inside the notebook

# Cell A: run test_video.py.main() with argv
import sys
from pathlib import Path
import runpy
import importlib.util

# 1) point to your repo root (change if different)
repo_root = Path("/home/work/dcvc/dcvc_try_p").resolve()
if str(repo_root) not in sys.path:
    sys.path.insert(0, str(repo_root))
print("repo_root on sys.path:", repo_root)

# 2) set command-line args (same as before)
sys.argv = [
    "test_video.py",
    "--model_path_i", "./checkpoints/cvpr2025_image.pth.tar",
    "--model_path_p", "./checkpoints/cvpr2025_video.pth.tar",
    "--rate_num", "4",
    "--test_config", "./dataset_config_example_yuv420.json",
    "--cuda", "1",
    "-w", "1",
    "--write_stream", "1",
    "--force_zero_thres", "0.12",
    "--output_path", "output.json",
    "--force_intra_period", "-1",
    "--reset_interval", "64",
    "--force_frame_num", "-1",
    "--check_existing", "0",
    "--verbose", "0",
]

# 3) load the module namespace from the file
module_globals = runpy.run_path(str(repo_root / "test_video.py"))

# 4) fetch main and run it
if "main" in module_globals and callable(module_globals["main"]):
    print("Calling test_video.main() now — output will appear below.")
    # call main and capture exit
    try:
        module_globals["main"]()
    except SystemExit as e:
        print("test_video exited with SystemExit:", e)
    except Exception as e:
        print("test_video raised an exception:", repr(e))
else:
    raise RuntimeError("test_video.py did not expose a callable 'main()' in the top-level namespace.")

Notes:

This runs the script in-process (not via a subprocess). If the script calls os._exit() or does aggressive process management the notebook kernel could be affected.

If you prefer launching a subprocess (safer isolation), replace steps 3–4 with a subprocess.run([...]) call — I can provide that if you prefer.



---

Cell B (optional) — try compiling the CUDA extension (build in-place)

# Cell B: (optional) try to compile the CUDA inference extension in-place.
# Run this only if you have CUDA toolkit installed and a compatible compiler.
import subprocess, sys, os
from pathlib import Path

repo_root = Path("/home/work/dcvc/dcvc_try_p").resolve()
ext_dir = repo_root / "src" / "layers" / "extensions" / "inference"

if not ext_dir.exists():
    print("Extension directory not found:", ext_dir)
else:
    print("Building extension in:", ext_dir)
    # run python setup.py build_ext --inplace
    cmd = [sys.executable, "setup.py", "build_ext", "--inplace"]
    proc = subprocess.run(cmd, cwd=str(ext_dir), capture_output=True, text=True)
    print("returncode:", proc.returncode)
    print("stdout:\n", proc.stdout[:1000])
    print("stderr:\n", proc.stderr[:2000])
    if proc.returncode == 0:
        print("Build successful. Re-run Cell A to attempt loading the CUDA implementation.")
    else:
        print("Build failed. Check stderr above. You can continue using the PyTorch fallback.")

What to expect:

If compilation succeeds, the repository will produce a compiled shared object (e.g., .so) in the extension folder and the message cannot import cuda implementation... fallback should disappear next time you run the script (it will use the compiled native extension).

If compilation fails, the script will continue to use the Python fallback (which you already ran).



---

If you want, tell me whether you prefer to run the script in-process (Cell A) or spawn a subprocess for full isolation. If you want the subprocess option I’ll paste that replacement cell (it’s safer for long-running tasks).

