# Patch training.py: add encode_decode_sequence wrapper and replace run_epoch with a version
# that uses compress()/decompress() when available.
from pathlib import Path
import re, textwrap

p = Path("training.py")
txt = p.read_text()

# Helper to insert: encode_decode_sequence and new run_epoch
insert_code = r'''
# ----------------------------
# Helper: run model's compress/decompress for a temporal sequence (video models like DMC)
# ----------------------------
def encode_decode_sequence(model, seq, qp=0):
    """
    seq: [B,T,C,H,W] torch tensor on correct device
    Returns dict: {'x_hat': [B,T,C,H,W] tensor, 'bpp': scalar-tensor}
    """
    device = seq.device
    B, T, C, H, W = seq.shape

    # ensure entropy coder and estimators are initialized
    try:
        model.update()
    except Exception:
        # If update fails, continue â€” model may still work
        print("Warning: model.update() failed or is not needed.")

    # Clear dpb and set initial reference frame (zeros) so apply_feature_adaptor won't crash
    try:
        model.clear_dpb()
    except Exception:
        pass

    zeros = torch.zeros((B, C, H, W), device=device, dtype=seq.dtype)
    try:
        model.add_ref_frame(feature=None, frame=zeros, increase_poc=False)
    except Exception:
        # If add_ref_frame signature is different, try to create a simple dpb element
        try:
            model.dpb = []
            model.add_ref_frame(frame=zeros, increase_poc=False)
        except Exception:
            # Give up gracefully
            print("Warning: failed to initialize dpb via add_ref_frame; proceeding (may error).")

    total_bits = 0
    x_hat_list = []
    for t in range(T):
        x = seq[:, t]  # [B,C,H,W]

        # compress -> returns dict with 'bit_stream' or other keys
        try:
            out = model.compress(x, qp)
        except Exception as e:
            # compress failed; warn and fallback to identity recon
            print("Warning: model.compress() failed during training wrapper:", e)
            # fallback: run model's internal decoder if available, else use input as reconstruction
            try:
                # try to call decoder modules directly (best-effort)
                with torch.no_grad():
                    # Call encoder+decoder path when available (best-effort)
                    feature = model.apply_feature_adaptor() if hasattr(model, "apply_feature_adaptor") else None
                    ctx, ctx_t = model.feature_extractor(feature, model.q_feature[0:1]) if hasattr(model, "feature_extractor") else (None, None)
                    y = model.encoder(x, ctx, model.q_encoder[0:1]) if hasattr(model, "encoder") else None
                    # try to run hyper/hyper_decoder and decoder path to get y_hat
                    y_hat = y
                    if hasattr(model, "decoder") and y_hat is not None:
                        # try best-effort decode
                        x_hat, _feat = model.get_recon_and_feature(y_hat, ctx, model.q_decoder[0:1], model.q_recon[0:1])
                    else:
                        x_hat = x
                bits = 0
            except Exception:
                x_hat = x
                bits = 0
            x_hat_list.append(x_hat.unsqueeze(1))
            total_bits += bits
            continue

        # get bit stream bytes (may be None)
        bs = out.get("bit_stream", None)
        # if compress returned x_hat directly (some image models do), pick it
        x_hat = out.get("x_hat", None)
        if bs is None:
            # No bitstream; maybe single-frame image model returned x_hat; handle it
            if x_hat is None:
                # fallback: use identity
                x_hat = x
                bits = 0
            else:
                bits = 0
        else:
            # bs may be bytes or bytearray
            if isinstance(bs, (bytes, bytearray)):
                bits = len(bs) * 8
            elif hasattr(bs, "__len__"):
                try:
                    bits = len(bs) * 8
                except Exception:
                    bits = 0
            else:
                bits = 0

            # decompress to get reconstruction
            try:
                sps = {'height': H, 'width': W, 'ec_part': 0}
                dec = model.decompress(bs, sps, qp)
                x_hat = dec.get('x_hat', x)
            except Exception as e:
                # decompress failed, fallback to input
                print("Warning: model.decompress() failed inside training wrapper:", e)
                x_hat = x

        x_hat_list.append(x_hat.unsqueeze(1))
        total_bits += bits

    x_hat_seq = torch.cat(x_hat_list, dim=1) if len(x_hat_list) > 0 else seq
    # normalize bpp per-frame with same denom used elsewhere in training.py (num_pixels = B*H*W)
    denom = float(B * H * W) if (B * H * W) > 0 else 1.0
    bpp_val = float(total_bits) / denom
    # return bpp as a scalar tensor on device
    bpp_tensor = torch.tensor(bpp_val, device=device, dtype=torch.float32)
    return {'x_hat': x_hat_seq, 'likelihoods': None, 'bpp_tensor': bpp_tensor}
'''

# Now replace the existing run_epoch function with a patched version that uses the helper
# Find start of def run_epoch and end (we assume it ends before the next def or main)
run_epoch_pattern = r"def run_epoch\(.*?\):\n(?:.*?\n)*?(?=\n# ----------------------------|\ndef |\nif __name__)"
# Build new run_epoch function text (keeps previous behavior, but uses encode_decode_sequence when model supports compress)
new_run_epoch = r'''
def run_epoch(model, loader, optimizer, scaler, device, args, epoch, is_train=True):
    model.train() if is_train else model.eval()
    total_loss = 0.0
    total_dist = 0.0
    total_bpp = 0.0
    steps = 0
    startt = time.time()

    use_video_api = hasattr(model, "compress") and hasattr(model, "decompress")

    for it, seq in enumerate(loader):
        # seq: [B,T,3,H,W]
        seq = seq.to(device, non_blocking=True)
        B,T,C,H,W = seq.shape
        num_pixels = B * H * W  # normalized per-frame; keep same denom as previous code

        # run model: if video API available use encode/decode wrapper, else call model(seq)
        if use_video_api:
            out = encode_decode_sequence(model, seq, qp=0)
            x_hat = out.get('x_hat')
            likelihoods = out.get('likelihoods', None)
            # bpp tensor provided directly by wrapper
            bpp = out.get('bpp_tensor', torch.tensor(0.0, device=device))
            # ensure shapes align: x_hat [B,T,C,H,W]
        else:
            try:
                with autocast(enabled=args.amp):
                    out = model(seq)
            except Exception:
                # fallback: try to run per-frame image model
                flat = seq.view(B*T, C, H, W)
                with autocast(enabled=args.amp):
                    out_flat = model(flat)
                if isinstance(out_flat, dict):
                    x_hat_flat = out_flat.get('x_hat') or out_flat.get('recon') or out_flat.get('x_rec')
                    likelihoods = out_flat.get('likelihoods') or out_flat.get('y_likelihoods') or out_flat.get('lik')
                elif isinstance(out_flat, (list, tuple)):
                    x_hat_flat = out_flat[0]
                    likelihoods = out_flat[1] if len(out_flat) > 1 else None
                else:
                    x_hat_flat = None
                if x_hat_flat is not None and x_hat_flat.dim() == 4 and x_hat_flat.shape[0] == B*T:
                    x_hat = x_hat_flat.view(B, T, C, H, W)
                else:
                    x_hat = seq.clone()
                bpp = torch.tensor(0.0, device=device)

        # if not set yet (video wrapper set it), and out is dict from model
        if not use_video_api:
            if isinstance(out, dict):
                x_hat = out.get('x_hat') or out.get('recon') or out.get('x_rec')
                likelihoods = out.get('likelihoods') or out.get('y_likelihoods') or out.get('lik')
                if x_hat is not None and x_hat.dim() == 4 and x_hat.shape[0] == B*T:
                    x_hat = x_hat.view(B, T, C, H, W)
            elif isinstance(out, (list, tuple)):
                x_hat = out[0]
            else:
                # ensure we have x_hat
                x_hat = seq.clone()

            if 'bpp_tensor' in out:
                bpp = out['bpp_tensor']
            else:
                # compute bpp from likelihoods if available
                bpp = compute_bpp_from_likelihoods(likelihoods, num_pixels).to(device)

        # MSE over all frames and channels
        dist = nn.functional.mse_loss(x_hat, seq, reduction='mean')
        # if bpp is a tensor scalar or a python float
        if isinstance(bpp, torch.Tensor):
            bpp_val = bpp
        else:
            bpp_val = torch.tensor(float(bpp), device=device)

        loss = dist + args.lambda_rd * bpp_val

        if is_train:
            optimizer.zero_grad()
            scaler.scale(loss).backward()
            if args.max_norm > 0:
                scaler.unscale_(optimizer)
                torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_norm)
            scaler.step(optimizer)
            scaler.update()

        total_loss += float(loss.detach().cpu().item())
        total_dist += float(dist.detach().cpu().item())
        total_bpp += float(bpp_val.detach().cpu().item())
        steps += 1

        if is_train and it % args.log_interval == 0:
            print(f"{'Train' if is_train else 'Val'} Epoch {epoch} it {it}/{len(loader)} loss={loss.item():.6f} dist={dist.item():.6f} bpp={bpp_val.item():.6f}")

    elapsed = time.time() - startt
    avg_loss = total_loss / max(1, steps)
    avg_dist = total_dist / max(1, steps)
    avg_bpp = total_bpp / max(1, steps)
    print(f"{'Train' if is_train else 'Val'} Epoch {epoch} finished: avg_loss={avg_loss:.6f} avg_dist={avg_dist:.6f} avg_bpp={avg_bpp:.6f} throughput_steps_per_sec={steps/max(1e-6,elapsed):.2f}")
    return avg_loss, avg_dist, avg_bpp
'''

# Insert helper function after imports: look for the comment "# ----------------------------\n# bpp helper" and insert above it
if "def encode_decode_sequence" not in txt:
    # choose insertion point: after compute_bpp_from_likelihoods definition end
    insert_after = "def compute_bpp_from_likelihoods"
    if insert_after in txt:
        # find location of compute_bpp_from_likelihoods function end by searching the next "# ----------------------------"
        parts = txt.split("# ----------------------------")
        # reassemble with helper inserted before the training/validation epoch section
        # simpler: insert helper before the line "def run_epoch"
        txt = re.sub(r"(\n)def run_epoch", "\n" + insert_code + "\n\ndef run_epoch", txt, count=1)
    else:
        # fallback: append at top
        txt = insert_code + "\n\n" + txt

# Now replace the whole run_epoch block if present
# Find the start index of "def run_epoch" and the start of the next top-level def or if __name__
m = re.search(r"def run_epoch\(.*\):", txt)
if m:
    start = m.start()
    # find the place to stop: locate "def main()" or "if __name__"
    end_match = re.search(r"\ndef main\(", txt[start:])
    if end_match:
        end = start + end_match.start()
        txt_new = txt[:start] + new_run_epoch + txt[end:]
    else:
        # fallback: replace until end of file
        txt_new = txt[:start] + new_run_epoch
    p.write_text(txt_new)
    print("Patched training.py run_epoch and inserted encode_decode_sequence. Showing run_epoch header:\n")
    # show snippet
    print("\n".join(txt_new.splitlines()[start:start+80]))
else:
    print("Could not find run_epoch in training.py; please inspect training.py manually.")
