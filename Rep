Nice debugging — you hit the classic case where the entropy/C++ coder isn’t available, so model.update() failed, model.compress() returned None (or raised), and the wrapper fell back to a path that produced no gradient. That made the loss a tensor with no grad_fn and backward() failed:

RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn

Fix: when model.compress() fails, we must produce a differentiable reconstruction that depends on the model weights so gradients flow. I’ll give you a safe patch that:

keeps trying model.compress() / model.decompress() when available,

if that fails (entropy coder missing), computes an approximate reconstruction using the model’s encoder/decoder submodules (no rounding / entropy coding) — this uses model parameters so gradients exist, and training proceeds,

still sets bits = 0 (we can’t compute true bpp without the coder), so training becomes RD with bpp=0 (i.e. pure distortion) until coder is available.


Do this now: run the single notebook cell below which patches training.py’s encode_decode_sequence fallback to produce differentiable reconstructions. After it runs, re-run training exactly the same way you did (I include the command using the same Python executable and PYTHONPATH).


---

Patch training.py (one cell — copy & paste into a notebook cell and run)

from pathlib import Path
import re, textwrap

p = Path("training.py")
txt = p.read_text()

# find encode_decode_sequence definition start
if "def encode_decode_sequence(model, seq, qp=0):" not in txt:
    print("encode_decode_sequence not found in training.py — aborting patch.")
else:
    # Replace the compress-exception fallback block with a differentiable fallback.
    # We'll search for the block that says "except Exception as e:" and includes the no_grad fallback, and replace it.
    new_block = textwrap.dedent(r'''
        except Exception as e:
            # compress failed; warn and fallback to a differentiable decoder path that uses model parameters
            print("Warning: model.compress() failed during training wrapper:", e)
            # Build a differentiable reconstruction using encoder/decoder submodules (best-effort).
            try:
                # Do NOT use torch.no_grad() here — we need gradients to flow to model params.
                # Use model.feature_adaptor_i / feature_extractor / encoder / decoder / recon_generation_net
                # Many models define these submodules (DMC). This path avoids entropy coder.
                device = x.device
                # prepare feature adaptor input: pixel-unshuffle of reference frame or of current input
                # If dpb exists and has a feature, use it, else use pixel_unshuffle of current frame
                try:
                    ref_feature = None
                    if hasattr(model, "dpb") and len(model.dpb) > 0 and model.dpb[0].feature is not None:
                        ref_feature = model.dpb[0].feature
                    if ref_feature is None:
                        ref_input = F.pixel_unshuffle(x, 8)
                        feature_adapted = model.feature_adaptor_i(ref_input) if hasattr(model, "feature_adaptor_i") else None
                    else:
                        feature_adapted = model.feature_adaptor_p(ref_feature) if hasattr(model, "feature_adaptor_p") else ref_feature
                except Exception:
                    # fallback to simple adaptor call
                    try:
                        feature_adapted = model.feature_adaptor_i(F.pixel_unshuffle(x, 8))
                    except Exception:
                        feature_adapted = None

                # compute ctx and ctx_t if possible
                ctx = None
                ctx_t = None
                try:
                    if feature_adapted is not None and hasattr(model, "feature_extractor"):
                        # choose a q_feature index safely (0 if available)
                        qf = model.q_feature[0:1] if hasattr(model, "q_feature") else None
                        ctx, ctx_t = model.feature_extractor(feature_adapted, qf)
                except Exception:
                    ctx = None
                    ctx_t = None

                # encoder -> y (no quantization)
                y = None
                try:
                    q_enc = model.q_encoder[0:1] if hasattr(model, "q_encoder") else None
                    if ctx is not None and hasattr(model, "encoder"):
                        y = model.encoder(x, ctx, q_enc)
                    elif hasattr(model, "encoder"):
                        # attempt encoder with zeros ctx
                        zeros_ctx = torch.zeros((x.size(0), getattr(model, "g_ch_d", 256), 1, 1), device=x.device) if False else None
                        if zeros_ctx is not None:
                            y = model.encoder(x, zeros_ctx, q_enc)
                        else:
                            y = model.encoder(x, ctx, q_enc)
                except Exception:
                    y = None

                # try to get y_hat either via a simple identity or via prior ops
                y_hat = y if y is not None else None

                # decoder -> feature -> recon
                if y_hat is not None and hasattr(model, "decoder") and hasattr(model, "recon_generation_net"):
                    q_dec = model.q_decoder[0:1] if hasattr(model, "q_decoder") else None
                    q_recon = model.q_recon[0:1] if hasattr(model, "q_recon") else None
                    try:
                        feature_dec = model.decoder(y_hat, ctx, q_dec)
                        x_hat = model.recon_generation_net(feature_dec, q_recon).clamp_(0, 1)
                    except Exception:
                        # fallback: try a simpler path using decoder or recon only
                        try:
                            x_hat = model.recon_generation_net(y_hat, q_recon).clamp_(0, 1)
                        except Exception:
                            x_hat = x  # last resort (this will be non-differentiable for model params, but unlikely)
                else:
                    # if we couldn't build decoder path, try calling model.get_recon_and_feature
                    try:
                        q_dec = model.q_decoder[0:1] if hasattr(model, "q_decoder") else None
                        q_recon = model.q_recon[0:1] if hasattr(model, "q_recon") else None
                        # attempt to synthesize y_hat via encoder results if needed
                        if y is None:
                            # try a light-weight conv path via feature_adapted -> feature_extractor forward_part2 -> decoder
                            if feature_adapted is not None and hasattr(model, "feature_extractor"):
                                c1, c2 = model.feature_extractor.forward_part1(feature_adapted, model.q_feature[0:1]) if hasattr(model.feature_extractor, "forward_part1") else (None, None)
                                ctx_try = model.feature_extractor.forward_part2(c1) if c1 is not None else None
                                # attempt to produce a rough reconstruction using decoder if possible
                                if hasattr(model, "decoder") and ctx_try is not None:
                                    # create a small dummy y (zeros) with appropriate channels
                                    B,Ct,Ht,Wt = ctx_try.shape if ctx_try is not None else (x.size(0), x.size(1), x.size(2), x.size(3))
                                    y_dummy = torch.zeros((x.size(0), getattr(model, "g_ch_y", 128), Ht, Wt), device=x.device, dtype=x.dtype)
                                    try:
                                        feature_dec = model.decoder(y_dummy, ctx_try, q_dec)
                                        x_hat = model.recon_generation_net(feature_dec, q_recon).clamp_(0,1)
                                    except Exception:
                                        x_hat = x
                                else:
                                    x_hat = x
                            else:
                                x_hat = x
                        else:
                            # last attempt, construct via get_recon_and_feature
                            try:
                                x_hat, _ = model.get_recon_and_feature(y_hat, ctx, q_dec, q_recon)
                            except Exception:
                                x_hat = x
                    except Exception:
                        x_hat = x

            except Exception as e2:
                print("Warning: differentiable fallback also failed:", e2)
                x_hat = x
            bits = 0

This replaces the prior non-differentiable fallback (which used torch.no_grad() or returned a detached tensor) so training can compute gradients through the model parameters.

Apply the patch by running the cell above. After that, run training with the same interpreter and PYTHONPATH as before (copy-paste the cell below — you already used it successfully):


---

Re-run training (use same Python executable)

Run this in a notebook cell:

import sys, os
py = sys.executable
print("Running training with:", py)
os.system(f'PYTHONPATH=$PWD "{py}" training.py '
          '--train-glob "data/frames/train/**/*.png" '
          '--val-glob "data/frames/val/**/*.png" '
          '--patch-size 256 --temporal-len 4 --batch-size 6 '
          '--epochs 30 --cuda --amp '
          '--pretrained "./checkpoints/cvpr2025_video.pth.tar" '
          '--lambda-rd 0.01 '
          '--save-dir "./checkpoints_finetune_lambda0.01" '
          '--log-dir "./logs/dcvc_finetune_lambda0.01" '
          '--workers 4')


---

Why this is safe

The differentiable fallback uses model submodules (encoder/decoder/recon) so gradients flow to model parameters even when the entropy coder is missing.

We set bits = 0 because without the RANS/entropy coder we cannot compute true bpp; this effectively trains on distortion (MSE) only. That’s acceptable for initial finetuning — when the C++ extensions are available later you can re-enable the entropy path and train with full RD loss.

This is a non-invasive patch — it only changes the fallback behavior inside encode_decode_sequence.



---

If after patching you see any new tracebacks from the differentiable fallback (e.g., attribute missing or shape mismatch), paste the full traceback here and I’ll refine the fallback to match the model specifics. If the training now runs, I’ll also show how to verify training quality (compute PSNR/SSIM on validation clips and visualize reconstructions).

